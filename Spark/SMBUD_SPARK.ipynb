{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d120afb9-fe20-4bf8-9998-51bfcefe12ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "771d0a08-c7fc-40ba-a469-6bce668947df",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"SMBUD_project\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f7d327-0196-4366-b1d3-9114db1cdda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define article custom schema\n",
    "schemaArticle = StructType([\n",
    "\tStructField('_id', StringType(), True),\n",
    "\tStructField('title', StringType(), True),\n",
    "\tStructField('authors',\n",
    "\t\tArrayType(\n",
    "\t\tStructType([\n",
    "\t\t\t StructField('idAuth', StringType(), True),\n",
    "\t\t\t StructField('org', StringType(), True)\n",
    "\t\t]), True)\n",
    "\t),\n",
    "\tStructField('n_citation', IntegerType(), True), \n",
    "\tStructField('abstract', StringType(), True), \n",
    "\tStructField('doi', StringType(), True),\n",
    "\tStructField('keywords', ArrayType(StringType()), True),\n",
    "\tStructField('isbn', StringType(), True),\n",
    "\tStructField('page_start', StringType(), True),\n",
    "\tStructField('page_end', StringType(), True),\n",
    "\tStructField('year', IntegerType(), True),\n",
    "\tStructField('fos', ArrayType(StringType()), True),\n",
    "\tStructField('references', ArrayType(StringType()), True),\n",
    "\tStructField('venue',\n",
    "\t\tStructType([\n",
    "\t\t\t StructField('raw', StringType(), True),\n",
    "\t\t\t StructField('type', IntegerType(), True),\n",
    "\t\t\t StructField('issue', StringType(), True),\n",
    "\t\t\t StructField('volume', StringType(), True),\n",
    "\t\t\t StructField('publisher', StringType(), True)\n",
    "\t\t])\n",
    "\t),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68a9536e-d248-4dd9-9afd-64515f9a79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we decided to use import from schema to explicitly show data structure\n",
    "df_articles = spark.read.schema(schemaArticle).json(\"./dblp_sample_filtered_spark.json\", multiLine=True)\n",
    "\n",
    "df_articles = df_articles.withColumn('address', f.when(f.col('venue.raw') == 'ESA', 'ESA_conference').otherwise(f.col('venue.raw')))\n",
    "df_articles = df_articles.withColumn(\"venue\", f.col(\"venue\").dropFields(\"raw\"))\n",
    "df_articles = df_articles.withColumn(\"venue\", f.struct(\"venue.*\", f.col(\"address\").alias(\"raw\"))) \n",
    "df_articles = df_articles.drop(\"address\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbfae60",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deaef0ca-2e9f-4ba6-b3cc-9456c199502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#issue, volume and publisher attributes inside venue are moved back in the root structure and removed from the inner struct\n",
    "df_articles = df_articles.withColumn(\"issue\", f.col(\"venue.issue\")) \\\n",
    "\t\t\t\t\t\t.withColumn(\"volume\", f.col(\"venue.volume\")) \\\n",
    "\t\t\t\t\t\t.withColumn(\"publisher\", f.col(\"venue.publisher\")) \\\n",
    "\t\t\t\t\t\t.withColumn(\"venue\", f.col(\"venue\").dropFields(\"issue\", \"volume\", \"publisher\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd3e637-2843-4a34-b71d-9d5cea4e8efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VENUES COLLECTION\n",
    "#A new dataframe is created with attributes of venue and the _id of the article\n",
    "#then it is all grouped by venue attributes and a list of the articles id for each venue is created\n",
    "#finally we drop rows with null raw to delete inconsistent tuple\n",
    "df_venues = df_articles.select(\"venue.raw\", \"venue.type\", \"_id\") \\\n",
    "\t\t\t\t\t\t.groupBy(\"raw\", \"type\") \\\n",
    "\t\t\t\t\t\t.agg(f.collect_list(\"_id\").alias(\"artIds\")) \\\n",
    "\t\t\t\t\t\t.dropna(subset=[\"raw\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12ef4ef3-ee77-4742-b378-8a90b8ae4e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we can keep only the raw attribute of the venue\n",
    "df_articles = df_articles.withColumn(\"venue_raw\", f.col(\"venue.raw\")).drop(\"venue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8868f13b-8b1d-4b0e-8c0d-a7cb2721ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we now add a generated field inside venues collection\n",
    "#for each venue a random city is selected that should represent the place where the venue was held\n",
    "citiesList = [\"New York\", \"London\", \"Paris\", \"Berlin\", \"Madrid\", \"Rome\", \"Dublin\", \"Copenhagen\", \"Vienna\", \"Amsterdam\", \"Brussels\", \"Lisbon\", \"Prague\", \"Athens\", \"Budapest\", \"Warsaw\", \"Zurich\", \"Luxembourg\", \"Oslo\", \"Stockholm\", \"Helsinki\", \"Moscow\", \"Istanbul\", \"Kiev\", \"Minsk\", \"Belgrade\", \"Bucharest\", \"Sofia\", \"Tallinn\", \"Riga\", \"Vilnius\", \"Tbilisi\", \"Yerevan\", \"Baku\", \"Dubai\", \"Abu Dhabi\", \"Doha\", \"Manama\", \"Muscat\", \"Riyadh\", \"Jeddah\", \"Mecca\", \"Medina\", \"Kuala Lumpur\", \"Singapore\", \"Hong Kong\", \"Shanghai\", \"Beijing\", \"Tokyo\", \"Seoul\", \"Bangkok\", \"Manila\"]\n",
    "cities = f.array([f.lit(city) for city in citiesList])\n",
    "df_venues = df_venues.withColumn(\"city\", cities[(f.rand() * len(citiesList)).cast(\"int\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "586d555c-4412-471d-90dc-25c6fd9ada30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the schema for the DataFrame of Authors\n",
    "schemaAuthors = StructType([\n",
    "    StructField(\"_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"nationality\", StringType(), True),\n",
    "    StructField(\"articles\", ArrayType(StringType(), True), True),\n",
    "    StructField(\"bio\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"orcid\", StringType(), True),\n",
    "    StructField(\"dob\", TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a40b4b24-c483-4912-8278-7e98928f4a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUTHORS COLLECTION\n",
    "#We simply import from json with specified schema and the conversion from string to timestamp is applied\n",
    "df_authors = spark.read.schema(schemaAuthors).json(\"./dblp_sample_reverted_filtered_spark.json\", multiLine=True)\n",
    "df_authors = df_authors.withColumn(\"dateofbirth\", f.to_timestamp(df_authors[\"dob\"], \"yyyy-MM-dd'T'HH:mm:ss'Z'\")) \\\n",
    "\t\t\t\t\t\t.drop(\"dob\") \\\n",
    "\t\t\t\t\t\t.withColumnRenamed(\"dateofbirth\", \"dob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "396efc6f-7824-4e47-89cd-24b973d1cc0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- idAuth: string (nullable = true)\n",
      " |    |    |-- org: string (nullable = true)\n",
      " |-- n_citation: integer (nullable = true)\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- doi: string (nullable = true)\n",
      " |-- keywords: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- isbn: string (nullable = true)\n",
      " |-- page_start: string (nullable = true)\n",
      " |-- page_end: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- fos: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- references: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- issue: string (nullable = true)\n",
      " |-- volume: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- venue_raw: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- articles: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- bio: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- orcid: string (nullable = true)\n",
      " |-- dob: timestamp (nullable = true)\n",
      "\n",
      "root\n",
      " |-- raw: string (nullable = true)\n",
      " |-- type: integer (nullable = true)\n",
      " |-- artIds: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "+--------------------+----+--------------------+---------+\n",
      "|                 raw|type|              artIds|     city|\n",
      "+--------------------+----+--------------------+---------+\n",
      "|2006 IEEE Interna...|   0|[53e9a281b7602d97...|Hong Kong|\n",
      "|2007 IEEE Interna...|   0|[53e9a317b7602d97...| Istanbul|\n",
      "|2009 IEEE INTERNA...|   0|[53e99fd6b7602d97...|    Paris|\n",
      "|2010 Internationa...|   0|[53e99fddb7602d97...| New York|\n",
      "|25 Years of Model...|   0|[53e99fe9b7602d97...|Hong Kong|\n",
      "|2ND INTERNATIONAL...|null|[53e99f94b7602d97...|  Beijing|\n",
      "|                 4OR|   0|[53e99fe4b7602d97...| Budapest|\n",
      "|7TH INTERNATIONAL...|null|[53e9a31fb7602d97...|Abu Dhabi|\n",
      "|A Quarterly Journ...|   1|[53e99fe4b7602d97...|   Madrid|\n",
      "|              A2CWiC|   0|[53e99fddb7602d97...|  Yerevan|\n",
      "+--------------------+----+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_articles.printSchema()\n",
    "df_authors.printSchema()\n",
    "df_venues.printSchema()\n",
    "df_venues.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b438276-9744-4321-96d9-3daf1b9ee18d",
   "metadata": {},
   "source": [
    "# Data creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddddf77",
   "metadata": {},
   "source": [
    "##### 1 - Insert new author Emanuele Della Valle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cabc87d0-de4e-498a-bf66-4b827c2264a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create a new Row object with the values for the new author\n",
    "new_author = Row(\n",
    "    _id=\"638db170ae9ea0d19fad7a79\",              #??????????????????????????????????????\n",
    "    name=\"Emanuele Delle Valle \",\n",
    "    nationality=\"it\",\n",
    "    # Set values for any other required columns\n",
    "    articles=[],\n",
    "    bio=\"Emanuele Della Valle holds a PhD in Computer Science from the \\\n",
    "        Vrije Universiteit Amsterdam and a Master degree in Computer Science\\\n",
    "        and Engineering from Politecnico di Milano. He is associate professor\\\n",
    "        at the Department of Electronics, Information and Bioengineering of\\\n",
    "        the Politecnico di Milano.\",\n",
    "    email=\"emanuele.dellavalle@gmail.com \",\n",
    "    orcid=\"0000-0002-5176 -5885\",\n",
    "    dob= datetime.strptime(\"March 7, 1975\", \"%B %d, %Y\")  # Create a datetime object for the author's date of birth\n",
    ")\n",
    "\n",
    "# Add the new row to the DataFrame\n",
    "df_authors = df_authors.union(spark.createDataFrame([new_author], schema = schemaAuthors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb8c6a04-ef96-4454-b41c-f3b84d34e89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+--------+--------------------+--------------------+--------------------+-------------------+\n",
      "|                 _id|                name|nationality|articles|                 bio|               email|               orcid|                dob|\n",
      "+--------------------+--------------------+-----------+--------+--------------------+--------------------+--------------------+-------------------+\n",
      "|638db170ae9ea0d19...|Emanuele Delle Va...|         it|      []|Emanuele Della Va...|emanuele.dellaval...|0000-0002-5176 -5885|1975-03-07 00:00:00|\n",
      "+--------------------+--------------------+-----------+--------+--------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_authors.filter(f.col(\"_id\") == \"638db170ae9ea0d19fad7a79\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b63dc06",
   "metadata": {},
   "source": [
    "##### 2 - Insert new publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "378c58cb-c44f-4357-938b-783580ab8cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_authors =  [Row(\"638db170ae9ea0d19fad7a79\", \"Politecnico di Milano\"), Row(\"638db170ae9ea0d19fad7a7a\", \"Politecnico di Milano\")] #????????????????\n",
    "\n",
    "new_article = Row(\n",
    "    _id=\"638db237d794b76f45c77916\",\n",
    "    title=\"An extensive study of C-SMOTE, a Continuous Synthetic Minority Oversampling Technique for Evolving Data Streams\",\n",
    "    authors=new_authors,\n",
    "    n_citation=3,\n",
    "    abstract = \"Streaming Machine Learning (SML) studies algorithms that update their models,\\\n",
    "        given an unbounded and often non-stationary flow of data performing a single pass. Online \\\n",
    "        class imbalance learning is a branch of SML that combines the challenges of both class imbalance\\\n",
    "        and concept drift. In this paper, we investigate the binary classification problem by rebalancing\\\n",
    "        an imbalanced stream of data in the presence of concept drift, accessing one sample at a time.\",\n",
    "    doi=\"10.1016/j.eswa.2022.116630\",\n",
    "    keywords=[\"Evolving Data Stream\",\"Streaming\",\"Concept drift\",\"Balancing\"],\n",
    "    isbn=\"123-4-567-89012-3\",\n",
    "    page_start=\"39\",\n",
    "    page_end=\"46\",\n",
    "    year=2022,\n",
    "    fos=[\"Computer Science\",\"Stream Reasoning\",\"Big Data\"],\n",
    "    references=[\"53e99fe4b7602d97028bf743\",\"53e99fddb7602d97028bc085\"],\n",
    "    issue=\"1\",\n",
    "    volume=\"196\",\n",
    "    publisher=\"Elsevier\",\n",
    "    venue_raw=\"ESA\"\n",
    ")\n",
    "\n",
    "# Add the new row to the DataFrame\n",
    "df_articles = df_articles.union(spark.createDataFrame([new_article]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59135eb0-f892-45e5-9d59-80a1910b8391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+-----------------+----------+--------+----+--------------------+--------------------+-----+------+---------+---------+\n",
      "|                 _id|               title|             authors|n_citation|            abstract|                 doi|            keywords|             isbn|page_start|page_end|year|                 fos|          references|issue|volume|publisher|venue_raw|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+-----------------+----------+--------+----+--------------------+--------------------+-----+------+---------+---------+\n",
      "|638db237d794b76f4...|An extensive stud...|[{638db170ae9ea0d...|         3|Streaming Machine...|10.1016/j.eswa.20...|[Evolving Data St...|123-4-567-89012-3|        39|      46|2022|[Computer Science...|[53e99fe4b7602d97...|    1|   196| Elsevier|      ESA|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+-----------------+----------+--------+----+--------------------+--------------------+-----+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_articles.filter(f.col(\"_id\") == \"638db237d794b76f45c77916\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0099da3b",
   "metadata": {},
   "source": [
    "##### 3 - Insert new venue \"ESA\", assuming it is not present in the db yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b314f663-c6b7-44bb-a5db-e6203f2e42b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_venue = Row(                  \n",
    "    raw=\"ESA\", \n",
    "    type=1,\n",
    "    artIds=[\"638db237d794b76f45c77916\"],\n",
    "    city=\"Montreal\"\n",
    ")\n",
    "\n",
    "# Add the new row to the DataFrame\n",
    "df_venues = df_venues.union(spark.createDataFrame([new_venue]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "499ef802-20c7-417a-b192-1722f1db1309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+--------+\n",
      "|raw|type|              artIds|    city|\n",
      "+---+----+--------------------+--------+\n",
      "|ESA|   1|[638db237d794b76f...|Montreal|\n",
      "+---+----+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_venues.filter(f.col(\"raw\") == \"ESA\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a9f7a2",
   "metadata": {},
   "source": [
    "##### 4 - Adding the new article to both authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b70a1e1-de68-4cce-93c9-dfaed0a22de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the new article to the new author\n",
    "\n",
    "df_authors = df_authors.withColumn(\n",
    "    \"articles\",\n",
    "    f.when(f.col(\"_id\") == \"638db170ae9ea0d19fad7a79\",\n",
    "        f.array_union(df_authors.articles, f.array(f.lit(\"638db237d794b76f45c77916\"))))\\\n",
    "    .when(f.col(\"_id\") == \"638db170ae9ea0d19fad7a7a\",\n",
    "        f.array_union(df_authors.articles, f.array(f.lit(\"638db237d794b76f45c77916\"))))\n",
    "    .otherwise(f.col(\"articles\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db4b9bcb-b893-4a4e-b38f-5e4155ea5a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|                 _id|                name|nationality|            articles|                 bio|               email|               orcid|                dob|\n",
      "+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|638db170ae9ea0d19...|Emanuele Delle Va...|         it|[638db237d794b76f...|Emanuele Della Va...|emanuele.dellaval...|0000-0002-5176 -5885|1975-03-07 00:00:00|\n",
      "+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_authors.filter(f.col(\"_id\") == \"638db170ae9ea0d19fad7a79\").show() #checking only Emanuele della Valle since the other author hasn't been inserted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fedb261",
   "metadata": {},
   "source": [
    "##### 5 - Incrementing n_citations by 1 of cited articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c17bbcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                 _id|n_citation|\n",
      "+--------------------+----------+\n",
      "|53e99fe4b7602d970...|        12|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|                 _id|n_citation|\n",
      "+--------------------+----------+\n",
      "|53e99fddb7602d970...|         2|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking previous n_citation\n",
    "df_articles.filter(f.col(\"_id\") == \"53e99fe4b7602d97028bf743\").select(\"_id\",\"n_citation\").show()\n",
    "df_articles.filter(f.col(\"_id\") == \"53e99fddb7602d97028bc085\").select(\"_id\",\"n_citation\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f760314c-506e-47ed-b69a-71bcb7f0fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#increment number of citations\n",
    "df_articles = df_articles.withColumn(\n",
    "    \"n_citation\",\n",
    "    f.when(f.col(\"_id\") == \"53e99fe4b7602d97028bf743\",\n",
    "       df_articles.n_citation+1) \\\n",
    "    .when(f.col(\"_id\") == \"53e99fddb7602d97028bc085\",\n",
    "       df_articles.n_citation+1)   \n",
    "    .otherwise(f.col(\"n_citation\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbb4991b-0a8c-46fa-af47-0e9d5075b698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                 _id|n_citation|\n",
      "+--------------------+----------+\n",
      "|53e99fe4b7602d970...|        13|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|                 _id|n_citation|\n",
      "+--------------------+----------+\n",
      "|53e99fddb7602d970...|         3|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking updated n_citation\n",
    "df_articles.filter(f.col(\"_id\") == \"53e99fe4b7602d97028bf743\").select(\"_id\", \"n_citation\").show()\n",
    "df_articles.filter(f.col(\"_id\") == \"53e99fddb7602d97028bc085\").select(\"_id\", \"n_citation\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958b3c73-f2a5-4ede-b584-36ee3ca38586",
   "metadata": {},
   "source": [
    "# QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3915e05-0e7f-4e7b-9372-c104ec1c7908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+----+\n",
      "|               title| raw|type|\n",
      "+--------------------+----+----+\n",
      "|Locality Sensitiv...|ICDE|   0|\n",
      "+--------------------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#WHERE+JOIN - QUERY 1\n",
    "#Print the type of the venue of an article with a specific title\n",
    "df_articles.join(df_venues, df_articles.venue_raw == df_venues.raw, \"inner\")\\\n",
    "           .filter(f.col(\"title\") == \"Locality Sensitive Outlier Detection: A ranking driven approach\").select(\"title\", \"raw\", \"type\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4587ee58-c8e5-4280-8530-de0222d19c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+-----------------+----------+--------+----+--------------------+----------+-----+------+---------+----------------+\n",
      "|                 _id|               title|             authors|n_citation|            abstract|                 doi|            keywords|             isbn|page_start|page_end|year|                 fos|references|issue|volume|publisher|       venue_raw|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+-----------------+----------+--------+----+--------------------+----------+-----+------+---------+----------------+\n",
      "|53e99fd6b7602d970...|Editorial: The Te...|[{53f48cc4dabfaea...|        11|                null|10.1023/A:1022840...|  [machine learning]|978-1-5904-9884-3|       141|     144|1986|[Terminology, Com...|        []|    2|     1|     null|Machine Learning|\n",
      "|53e99fd6b7602d970...|Machine Learning,...|[{53f48bbadabfaea...|       127|                null|                null|  [machine learning]|             isbn|       198|     217|2005|                null|        []| null|   119|     null|            ICML|\n",
      "|53e99fe4b7602d970...|Medical Expert Ev...|[{53f455c7dabfaef...|         9|A set of machine ...|10.1007/3-540-399...|[existing knowled...|    3-540-41089-9|       159|     168|2000|[Coronary heart d...|        []| null|  1933|     null|           ISMDA|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+-----------------+----------+--------+----+--------------------+----------+-----+------+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#WHERE+LIMIT+LIKE - QUERY 2\n",
    "#Articles whose title string contains \"Machine Learning\" - limit 3\n",
    "df_articles.filter(f.col(\"title\").like(\"%Machine Learning%\")).limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5313e11c-f9a7-4424-8621-b25cab62b21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+-----+-------------------+\n",
      "|                 _id|                name|nationality|            articles|                 bio|               email|orcid|                dob|\n",
      "+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+-----+-------------------+\n",
      "|542a4c9fdabfae61d...|             Ye Wang|         dk|[53e99f86b7602d97...|Marcus J. Nadenau...|   Ye.Wang@gmail.com| null|1961-07-02 01:00:00|\n",
      "|53f48bc5dabfaea7c...|Srinivasan Partha...|         jp|[53e99f86b7602d97...|Marian Codreanu (...|Srinivasan.Partha...| null|1969-06-06 01:00:00|\n",
      "|53f44b6fdabfaec09...|   Shirish Tatikonda|         gr|[53e99f86b7602d97...|Dimitris Papadias...|Shirish.Tatikonda...| null|1950-01-10 01:00:00|\n",
      "|53f7f79fdabfae90e...|      Moshe Zukerman|         jp|[53e99f86b7602d97...|Shugong Xu [SM] (...|Moshe.Zukerman@gm...| null|1984-09-05 02:00:00|\n",
      "|53f434f5dabfaec09...|     Michael Wiegand|         jp|[53e99f86b7602d97...|Martin Stridh (S'...|Michael.Wiegand@y...| null|1955-06-23 01:00:00|\n",
      "|53f46724dabfaedd7...|          GeunSik Jo|         jp|[53e99f86b7602d97...|Chung-An Shen rec...|GeunSik.Jo@yahoo.com| null|1990-08-27 02:00:00|\n",
      "|53f431f6dabfaec09...|        Carla Achury|         gr|[53e99f86b7602d97...|Oscar Luaces rece...|Carla.Achury@yaho...| null|1989-02-07 01:00:00|\n",
      "|53f4d2f6dabfaef20...|        Kong-Aik Lee|         jp|[53e99f87b7602d97...|Yikun Yu received...|Kong-Aik.Lee@yand...| null|1947-09-13 02:00:00|\n",
      "|53f4562ddabfaee0d...|Shahram Shah-Heydari|         gr|[53e99f8cb7602d97...|Yakoub Bazi (S'05...|Shahram.Shah-Heyd...| null|1972-11-11 01:00:00|\n",
      "|53f4384ddabfaee4d...|         Wenfang Tan|         dk|[53e99f8cb7602d97...|Farid Melgani (M'...|Wenfang.Tan@yahoo...| null|1975-09-12 01:00:00|\n",
      "+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#WHERE+IN+NESTED_QUERY - QUERY 3\n",
    "#Find authors that has the same nationality of at least one of the authors of \"Locality Sensitive Outlier Detection: A ranking driven approach\" article\n",
    "\n",
    "#Create the list of nationalities of the article's authors\n",
    "nationalities_list = df_articles.filter(f.col(\"title\") == \"Locality Sensitive Outlier Detection: A ranking driven approach\")\\\n",
    "                            .select(f.explode(df_articles.authors.idAuth).alias(\"idAuth\"))\\\n",
    "                            .join(df_authors, on=f.col(\"idAuth\") == df_authors._id)\\\n",
    "                            .select(\"nationality\")\\\n",
    "                            .agg(f.collect_set(\"nationality\")).collect()[0][0]\n",
    "#find all the authors with the same nationalities of the authors of the initial article \n",
    "df_authors.filter(f.col(\"nationality\")\\\n",
    "          .isin(nationalities_list)).limit(10).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4b9d50f-b125-4548-99ec-f9cf939c1b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+\n",
      "|        keywords|n_occurences|\n",
      "+----------------+------------+\n",
      "|     data mining|          27|\n",
      "|computer science|          22|\n",
      "|        internet|          17|\n",
      "+----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GROUP_BY+JOIN+AS - QUERY 4\n",
    "#Print the 3 most frequent keywords of articles written by italian authors\n",
    "df_italian = df_authors.filter(f.col(\"nationality\") == \"it\")\\\n",
    "                       .select(f.explode(\"articles\")).withColumnRenamed(\"col\",\"articles\")\n",
    "df_italian = df_italian.groupby(\"articles\").count() #Dummy count\n",
    "\n",
    "df_keywords = df_italian.join(df_articles, df_italian.articles == df_articles._id, \"inner\")\\\n",
    "                        .select(\"articles\", f.explode(\"keywords\")).withColumnRenamed(\"col\",\"keywords\")\\\n",
    "                        .groupby(\"keywords\")\\\n",
    "                        .agg(f.count(\"keywords\").alias(\"n_occurences\"))\\\n",
    "                        .sort(\"n_occurences\", ascending=False)\\\n",
    "                        .limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b083d5ad-d135-40b8-aba5-c5fabf077dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      city|count|\n",
      "+----------+-----+\n",
      "|     Seoul|   73|\n",
      "|     Tokyo|   71|\n",
      "|    Berlin|   69|\n",
      "|Luxembourg|   68|\n",
      "|  Belgrade|   67|\n",
      "|    Jeddah|   66|\n",
      "|  Shanghai|   66|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#WHERE+GROUP_BY - QUERY 5\n",
    "#Print the cities with more than 65 venues\n",
    "df_venues \\\n",
    "    .groupby(\"city\")\\\n",
    "    .count()\\\n",
    "    .filter(f.col(\"count\") > 65)\\\n",
    "    .sort(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                 fos|n_occurence|\n",
      "+--------------------+-----------+\n",
      "|         Computation|        113|\n",
      "|            Test set|         16|\n",
      "| Operations research|         61|\n",
      "|         Game theory|         32|\n",
      "|       Mixture model|         24|\n",
      "|Bandwidth (signal...|         58|\n",
      "|          Annotation|         31|\n",
      "|Load balancing (c...|         29|\n",
      "|         Source code|         42|\n",
      "|     Word error rate|         16|\n",
      "|     Cloud computing|         57|\n",
      "|        Broadcasting|         25|\n",
      "|Knowledge management|        203|\n",
      "|           Test case|         17|\n",
      "|          Web server|         16|\n",
      "|    Nonlinear system|         72|\n",
      "|User experience d...|         18|\n",
      "|       Interpolation|         26|\n",
      "|            Data Web|         21|\n",
      "|Entropy (informat...|         27|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Query 6 GROUP BY +  HAVING + AS\n",
    "#find the field of studies that appers more than 15 times\n",
    "\n",
    "df_articles\\\n",
    "    .select(\"_id\", \"title\", f.explode(\"fos\")).withColumnRenamed(\"col\", \"fos\")\\\n",
    "    .groupby(\"fos\")\\\n",
    "    .agg(f.count(\"fos\").alias(\"n_occurence\"))\\\n",
    "    .filter(f.col(\"n_occurence\") > 15).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------+\n",
      "|           venue_raw|volume|num_articles|\n",
      "+--------------------+------+------------+\n",
      "|Applied Mathemati...|   218|           5|\n",
      "| Pattern Recognition|    45|           5|\n",
      "|  Expert Syst. Appl.|    39|           5|\n",
      "|Applied Mathemati...|   217|           5|\n",
      "|  IEICE Transactions|  97-A|           5|\n",
      "|  Expert Syst. Appl.|    37|           5|\n",
      "+--------------------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#QUERY7 WHERE + GROUP BY + HAVING + AS\n",
    "#Find all the volumes with at least 5 articles in this dataset published after 2000\n",
    "df_articles\\\n",
    "    .filter(f.col(\"year\") > 2000)\\\n",
    "    .groupby(\"venue_raw\", \"volume\")\\\n",
    "    .agg(f.count(\"volume\").alias(\"num_articles\"))\\\n",
    "    .filter(f.col(\"num_articles\") > 4)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "020a47f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-------------+-----------+-------------------------------------------------------------------------------------+\n",
      "|_id                     |name         |venue_count|venues_list                                                                          |\n",
      "+------------------------+-------------+-----------+-------------------------------------------------------------------------------------+\n",
      "|54055740dabfae44f0803fbb|Naohiro Ishii|3          |Las Vegas, NV - Honolulu, HI - International Journal on Artificial Intelligence Tools|\n",
      "+------------------------+-------------+-----------+-------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#QUERY 10 WHERE, GROUP BY, HAVING, 2 JOINS\n",
    "#Find all the authors that published on more than 2 Journals \n",
    "\n",
    "df_exploded_authors = df_authors.alias(\"auth\")\\\n",
    "                        .select(\"auth._id\",\"auth.name\", f.explode(\"auth.articles\").alias(\"article\"))\\\n",
    "                        .join(df_articles.alias(\"art\"), on=f.col(\"article\") == df_articles._id)\\\n",
    "                        .select(\"auth._id\",\"auth.name\",\"art._id\",\"art.venue_raw\")\\\n",
    "                        .join(df_venues.alias(\"ven\"), on=f.col(\"venue_raw\") == df_venues.raw)\\\n",
    "                        .filter(f.col(\"type\") == 1)\\\n",
    "                        .groupBy(\"auth._id\")\\\n",
    "                        .agg(f.first(\"name\").alias(\"name\"),f.countDistinct(\"raw\").alias(\"venue_count\"),f.concat_ws(\" - \",f.collect_set(\"raw\")).alias(\"venues_list\"))\\\n",
    "                        .filter(f.col(\"venue_count\") > 2)\\\n",
    "                        .orderBy(\"venue_count\", ascending=False).show(3,truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smbud",
   "language": "python",
   "name": "smbud"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
